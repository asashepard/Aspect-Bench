# Contributing to Aspect Bench

Thank you for your interest in contributing! This document provides guidelines for adding new repositories and creating quality benchmark tasks.

## Table of Contents

- [Adding a New Repository](#adding-a-new-repository)
- [Creating Benchmark Tasks](#creating-benchmark-tasks)
- [Task Validation Requirements](#task-validation-requirements)
- [Pull Request Process](#pull-request-process)

---

## Adding a New Repository

### Step 1: Register the Repository

Add your repository to the `REPO_REGISTRY` in `src/scripts/load_task_defs.py`:

```python
REPO_REGISTRY = {
    # ... existing repos ...
    "your-repo": {
        "name": "Human-Readable Name",
        "path": "your-repo",              # Folder name in repos/
        "backend_path": "src",            # Subdirectory with main code (or "" for root)
        "test_path": "tests",             # Where tests live (or "" if distributed)
        "language": "python",             # "python" or "typescript"
        "git_url": "https://github.com/owner/repo.git",
    },
}
```

### Step 2: Clone the Repository

```bash
cd repos
git clone https://github.com/owner/repo.git your-repo
cd ..
```

### Step 3: Generate Knowledge Base with Aspect Code Extension

The KB files are generated by the **Aspect Code VS Code extension**:

1. Open the cloned repository in VS Code
2. Ensure the [Aspect Code extension](https://marketplace.visualstudio.com/items?itemName=aspect-code.aspect-code) is installed
3. Run the extension to generate the `.aspect/` folder with KB files
4. Copy the generated KB to the benchmark:
   ```bash
   cp repos/your-repo/.aspect/kb.txt example_kb/kb_your-repo.txt
   ```

### Step 4: Create Harness Structure

Create the benchmark harness files in `src/repos/`:

```
src/repos/your-repo/
â”œâ”€â”€ prompts/                 # Generated prompts will go here
â”œâ”€â”€ tasks/
â”‚   â””â”€â”€ task_defs.yaml       # Task definitions
â””â”€â”€ tests/
    â””â”€â”€ conftest.py          # Test fixtures
```

---

## Creating Benchmark Tasks

### Task Definition Format

Create tasks in `src/repos/<repo>/tasks/task_defs.yaml`:

```yaml
tasks:
  - id: task-descriptive-name    # Use kebab-case, descriptive names
    name: "Short descriptive title"
    description: |
      Clear description of what the LLM should implement.
      Include:
      - The specific feature or fix needed
      - Expected behavior
      - Any constraints or edge cases
    
    category: "feature|bugfix|refactor|test"
    difficulty: "easy|medium|hard"
    
    files_to_modify:
      - "path/to/main_file.py"
      - "path/to/related_file.py"
    
    test_files:
      - "tests/test_feature.py"
    
    test_markers:
      - "test_specific_function"    # pytest -k marker
```

### Prompt Format

**Baseline prompt** (`prompts/baseline/<task_id>.txt`):
```
You are an expert software developer. Complete the following task:

[Task description from task_defs.yaml]

Modify the following files as needed:
- path/to/file.py

Respond with the complete modified file contents.
```

**Aspect KB prompt** (`prompts/aspect_kb3/<task_id>.txt`):
```
You are an expert software developer working on [Project Name].

## Project Context
[Include relevant KB content from example_kb/kb_*.txt]

## Task
[Task description]

## Files to Modify
- path/to/file.py

Respond with the complete modified file contents.
```

---

## Task Validation Requirements

**Every task MUST have:**

### 1. At Least 2 Passing Tests (Before Changes)
These tests verify baseline functionality and catch regressions:
```python
def test_existing_behavior():
    """This should pass before AND after changes."""
    assert existing_feature() == expected_value
```

### 2. At Least 2 Failing Tests (Before Changes)
These tests define the success criteria for the task:
```python
def test_new_feature():
    """This should FAIL before changes, PASS after."""
    assert new_feature() == expected_value  # Fails initially
```

### Validation Checklist

Before submitting a task:

- [ ] Clone fresh repo and run tests - verify 2+ pass, 2+ fail
- [ ] Run benchmark with `--tasks <task_id>` to test single task
- [ ] Verify LLM can parse the prompt and produce valid code
- [ ] Confirm tests pass after LLM applies changes
- [ ] Check for regressions (existing tests still pass)

### Testing Your Task

```bash
# Test single task
python src/scripts/run_benchmark.py --repo your-repo --tasks your-task-id --provider anthropic

# Verify test status
python src/scripts/run_tests_for_task.py --repo your-repo --task your-task-id
```

---

## Pull Request Process

### 1. Fork and Clone

```bash
git clone https://github.com/YOUR_USERNAME/aspect-code-bench.git
cd aspect-code-bench
```

### 2. Create a Branch

```bash
git checkout -b add-repo-name
# or
git checkout -b add-task-descriptive-name
```

### 3. Make Your Changes

- Add repository to registry
- Create task definitions in `tasks/task_defs.yaml`
- Generate KB using Aspect Code extension
- Generate prompts: `python src/scripts/generate_prompts.py`
- Validate all tasks pass requirements

### 4. Run Full Validation

```bash
# Run all tasks for your repo
python src/scripts/run_benchmark.py --repo your-repo --provider anthropic

# Generate report
python src/scripts/generate_report.py --experiment-id <id_from_run>
```

### 5. Submit PR

Include in your PR description:
- Brief description of the repository/tasks added
- Validation results (pass/fail counts)
- Any special considerations or dependencies

---

## Code Style

- Python: Follow PEP 8
- Use descriptive variable and function names
- Add docstrings to functions
- Keep tasks focused and atomic (one feature per task)
- Use kebab-case for task IDs (e.g., `add-csv-export`, not `add_csv_export`)

## Questions?

Open an issue for:
- Help with task creation
- Questions about repository compatibility
- Bug reports or feature requests

Thank you for contributing! ðŸš€
